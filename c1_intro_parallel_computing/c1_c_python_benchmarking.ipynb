{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4507b8d4-cc03-437c-9fb1-68001236fc83",
   "metadata": {},
   "source": [
    "# Benchmarking in Python\n",
    "\n",
    "Now, we want to be able to measure some performance statistics to truly prove that we have improved our software application's performance over many conditions, not just the perfect condition.\n",
    "\n",
    "This means that we'd like to send our code out as an actual job on DevCloud (to simulate dedicated, exclusive server processes), rather than just testing within the JupyterLab development environment.  In this notebook, we'll cover the different benchmarking interfaces that Python has.  \n",
    "\n",
    "If you are using a different language, feel free to skip or skim through this notebook so you can verify that you understand your language's benchmarking interfaces.\n",
    "\n",
    "We'll cover the Linux systemwide benchmark interfaces in Component 3.\n",
    "\n",
    "## Benchmarking interfaces in Python\n",
    "\n",
    "There are a few types of benchmarking interfaces we can use:\n",
    "\n",
    "| Interface | When to use |\n",
    "|-----------|-------------|\n",
    "| `time` | The base library that keeps track of time.  You can use `time.process_time()` to get CPU time so far and `time.perf_counter()` to get Wall time.  A simple subtraction of points marked by calls to `time.perf_counter()` can get you a quick estimation of your run time.  |\n",
    "| `timeit` | A built-in library dedicated to measuring Python code timing, complete with repeats.  |\n",
    "| IPython/Jupyter `%%timeit` | A modified interface that allows for advanced timing.  Can only run in the ipython interpreter or in a Jupyter notebook. |\n",
    "| `tracemalloc` | A built-in library dedicated to measuring and tracing memory blocks allocated by Python. |\n",
    "| `cProfile` | The Python profiler, which does not technically benchmark but can help determine any functions that run too often or take too much time. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a273959-6840-49dd-871e-c69bdb644bb5",
   "metadata": {},
   "source": [
    "## Using `timeit` to get truer results\n",
    "\n",
    "If we just want a quick check, we can use the counters in the `time` module to see how quickly our code runs.  However, this is usually just one run, which may be confounded by cache misses and other miscellaneous events that cause our program to run slower than the second or subsequent runs while our program is active.\n",
    "\n",
    "Thus, we can use the `timeit` library instead, so the same version of our code is repeated a few times such that we can get a truer minimum.  As noted in the [Python documentation](https://docs.python.org/3/library/timeit.html#timeit.Timer.repeat), the minimum value if all we need.\n",
    "\n",
    "Let's try using `timeit` to time an example program's `main()` function.  It is important that the `main()` function does not take any input parameters (arguments), otherwise we will have to use the `setup` input parameter for `timeit.timeit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533ba1b-f6f4-47ab-abad-58717ffaa414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_sum(in_array: np.ndarray) -> float:\n",
    "    output = 0.0\n",
    "    for item in in_array:\n",
    "        output += item\n",
    "    return output\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    example_data = np.linspace(start=-23.4, stop=72.4, num=200)\n",
    "    compute_sum(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24397988-55e2-4a99-9890-b44d46680f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "timer_main = timeit.Timer(main)\n",
    "timer_main.repeat(repeat=10, number=10)  # 10 measurements of 10 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdd6a5-80a6-4b56-9ee2-d4261844409d",
   "metadata": {},
   "source": [
    "Examine the list (array) returned by the `timeit.Timer.repeat()` function.  Notice how the first couple of runs had longer times!  If you were looking at the long-term bounds of execution wall time, then you'd just pick the minimum as the documentation says, because your program should have either exclusive CPU cores or program code cached.\n",
    "\n",
    "We could get rid of cache effects by using a random number generator instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c1786-4e6c-4358-bdde-98cf9eadf81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_main() -> None:\n",
    "    example_data = np.random.default_rng().uniform(low=-24.5, high=72.4, size=200)\n",
    "    compute_sum(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5af59-3a18-43fc-829b-cd6e5004197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_random_main = timeit.Timer(random_main)\n",
    "timer_random_main.repeat(repeat=10, number=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a05d6-1e28-4019-b8b9-9dace3349219",
   "metadata": {},
   "source": [
    "We can see that there's definitely some more variance (likely from the generation of the random numbers, which is quite slow when using just NumPy), but overall, it follows the same pattern of the first run being much longer than the rest of the runs.  The minimum run time is pretty much the same even with random numbers, so we can reasonably assume that we have an accurate timing of our program. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ae76f-1740-43cd-8d34-7319fb404270",
   "metadata": {},
   "source": [
    "You're now ready to use the `timeit` interface!\n",
    "\n",
    "If you'd like more information, here are some additional resources:\n",
    "\n",
    "- https://docs.python.org/3/library/timeit.html\n",
    "- https://note.nkmk.me/en/python-timeit-measure/\n",
    "- https://cvw.cac.cornell.edu/python/timing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06633c75-9e7a-4ebc-8edc-26d4e945df8e",
   "metadata": {},
   "source": [
    "## Using `tracemalloc` to gather memory data\n",
    "\n",
    "The [`tracemalloc`](https://docs.python.org/3/library/tracemalloc.html) module allows us to see how much memory is used by our Python code.  Using the example functions above, let's get a glimpse for how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414fb36-94de-4fd3-ac87-61bcaa89d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc as tm\n",
    "\n",
    "tm.start()\n",
    "tm.reset_peak()\n",
    "tm.clear_traces()\n",
    "\n",
    "timer_random_main.repeat(repeat=10, number=10)\n",
    "\n",
    "current_size, peak_size = tm.get_traced_memory()\n",
    "\n",
    "print(f\"current size: {current_size / 1024} KiB; \"\n",
    "      f\"peak size: {peak_size / 1024} KiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad4e75a-0b9f-42f4-9667-2dc37627520f",
   "metadata": {},
   "source": [
    "As we can see, we can get the amount of memory used by the Python interpreter after code execution while waiting for additional data, but we can also get the maximal size of memory allocated by Python to run our code.\n",
    "\n",
    "The peak is what matters the most, as it represents the worst possible case of our application.  We use the `timeit` module so that we can run repeats such that we get absolutely the worst-case peak size.\n",
    "\n",
    "You're now ready to use the `tracemalloc` module!\n",
    "\n",
    "If you'd like more information about `tracemalloc`, here are some additional resources:\n",
    "\n",
    "- https://docs.python.org/3/library/tracemalloc.html\n",
    "- https://tech.buzzfeed.com/finding-and-fixing-memory-leaks-in-python-413ce4266e7d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aac58b-947b-455d-ac6c-96320a408514",
   "metadata": {},
   "source": [
    "## Using `cProfile` to receive granular details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0c9f7-93d7-4087-9961-e01c51b4b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "with cProfile.Profile() as profiler:\n",
    "    profiler.runcall(timer_random_main.repeat, repeat=10, number=10)\n",
    "    profiler.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aba61d-88eb-430b-8fa6-5f67738b5447",
   "metadata": {},
   "source": [
    "As you can see, `cProfile` is very helpful and can break code runs down for us!  We know the number of times a function was called, what its total time was (anything less than one second total is nearly negligible in most non-embedded settings).\n",
    "\n",
    "If you'd like more information, here are some additional resources:\n",
    "\n",
    "- https://docs.python.org/3/library/profile.html#module-cProfile\n",
    "- https://docs.python.org/3/library/profile.html#module-pstats\n",
    "- https://cerfacs.fr/coop/python-profiling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
